---
title: 'Stochastic Block Model: a Tool for investigating community structure in Mesotrophic
  Grassland.'
date: "2023-04-21"
output:
  html_document:    
    code_folding: hide
    df_print: paged
  pdf_document: default
fig.width: 6
fig.height: 4
editor_options: 
  markdown: 
    wrap: 72
---
```{r setup, include = FALSE, echo = FALSE, message = FALSE, error = FALSE, warnings = FALSE}
library("RMySQL")
library(tidyverse)
library(igraph)
library(tidygraph)
library(dplyr)
library(ggraph)
library(sbm)

# Functions
dbDisconnectAll <- function(){
  ile <- length(dbListConnections(MySQL())  )
  lapply( dbListConnections(MySQL()), function(x) dbDisconnect(x) )
  cat(sprintf("%s connection(s) closed.\n", ile))
}
```

# Part 1: The toolbox. 
## Motivation for Stochastic Block Model (SBM).
Streamside meadows in the Sussex Ouse catchment have been surveyed since
2006 by volunteers from The River Ouse Project, described in detail
[here](http://www.sussex.ac.uk/riverouse/about). The data (to 2019) are
in a database and available for analysis, but consist of more than 24000
records of about 300 plant species from 260 surveys. For this reason, an
unbiased summary avoiding preconceived ideas, arbitrary decisions or
reductionist intent is required. Stochastic Block Model appears to offer
this, in ways to be described later.

## Data recording
Sites were sampled and the plants recorded using at least 5 2m x 2m
quadrats at each survey, as specified for NVC surveys of mesotrophic
grassland (Rodwell ...). Each of the 260 sample sites was surveyed just
once, so in this context, "survey" is equivalent to "site" or
"location". There is no replication in the data set, and we treat it as
a synoptic view of meadows in the river Ouse catchment, ignoring any
temporal drift over the study period; as though all 1457 quadrats were
thrown on the same day.

## Analysis objectives and data pre-processing
We are interested in plants that occur together more or less frequently
than would be expected by chance. Pairwise contingency tables for every
combination of the plants that we encountered were assessed with
Fisher's exact test, returning a p_value for the significance of any
association, and also the odds-ratio, an estimate of the strength and
sign of the association (inclusive or exclusive). Plant pairs (diads)
for which the p-value was less than 0.05 were accepted for further
analysis. Arguably, this is an arbitrary decision; but it seems
conventional enough. Further, some diads with a low p-value had the
odds-ratio flagged as "Inf"; after some examination, these also were
excluded from further analysis.

The p-values and logarithms of the odds-ratios (lor) were retained, but
it should be understood that *the SBM itself has no knowledge of these
quantities*. They are intended to be useful for onwards
investigation. The only thing the SBM knows about is whether or not an
association was detected for each dyad.

## Data wrangling
### data wrangling: data extraction
Here's how to extract data from the database:
```{r data_extraction, error = FALSE, message = FALSE, warnings = FALSE, results="asis"}
# GET DATA FROM DB
# Remote DB with password
con <- dbConnect(MySQL(), 
                 user  = "guest",
                 password    = "guest",
                 dbname="meadows",
                 port = 3306,
                 host   = "sxouse.ddns.net")

q <- sprintf('select survey_id, species.species_name from surveys
      join quadrats on quadrats.survey_id = surveys_id
      join visit_dates on quadrats.vd_id = visit_dates.vds_id
      join records on records.quadrat_id = quadrats_id
      join species on species.species_id = records.species_id where species.species_id != 4 and
      major_nvc_community like "MG%%" and quadrat_size = "2x2";')

# NOTE the double %% to escape the % formatting character

rs1 = dbSendQuery(con, q)
d <- as_tibble(fetch(rs1, n=-1))
dbDisconnectAll()
rm(con, q, rs1)
library(knitr)
kable(d[1:8,],caption = "The extracted data; first 8 rows")
```
Notice the SQL query. It returns just two columns of data, an identifier for the survey site, and a list of species names. This is all the information we need for the SBM. Each row represents a record that the plant in question was found at the identified site. Naturally, it is a very long list, `r nrow(d)` items.

### Data wrangling: site occupancy.
The first step is to make a binary site occupancy table with a 1 at every site for which a plant was found at least once; 0 otherwiseData. 

It is possible that the data extraction step above may have failed for you if, for example, the DB server was not available. I have saved a previous extraction as hits.csv and start the analysis section with that file.
```{r site_occupancy, warnings = FALSE, messages = FALSE}
d <- read.csv("hits.csv")
# Count (n) hits for each species (columns) in each survey (rows)
d1 <- (d %>% select(survey_id, species_name)
      %>% group_by(survey_id, species_name)
      %>% summarise(n=n())
      %>% ungroup()
      %>% pivot_wider(names_from = species_name, values_from = n))
# At this point, d has the number of hits for each survey and species.
# Replace anything numeric with 1, and any NA with 0
d1 <- (d1 %>% select(-survey_id) %>% replace(., !is.na(.), 1)
      %>% replace(., is.na(.), 0)) # Replace NAs with 0)
kable(d1[1:8,1:3],caption = "Top left corner of the site occupancy table")
```
The full site occupancy table has `r dim(d1)[1]` rows (sites) and `r dim(d1)[2]` columns (plants).

### Data wrangling: Event (contingency) detection

Plants are taken pairwise and contingency tables constructed for each pair. This introduces combinatorial complexity into the analysis which can be expected to take a few seconds to complete.
The first step is to make lists of hit vectors for each plant. Here we drop site specific information.
```{r}
# What follows thanks to Brian Shalloway: 
# https://www.bryanshalloway.com/2020/06/03/tidy-2-way-column-combinations/#fn4
# I have retained his table names in acknowledgement. Besides which, they are usefully descriptive.
df_lists <- d1 %>%
  summarise_all(list) %>% 
  pivot_longer(cols = everything(), 
               names_to = "var", 
               values_to = "vector")
kable(df_lists[1:3,],caption = "Hit vectors")
```
Obviously with one vector for each of the `r dim(df_lists)[1]` species currently under consideration.

Next, things get combinatorial.
```{r}
df_lists_comb <- expand(df_lists,
                        nesting(var, vector),
                        nesting(var2 = var, vector2 = vector))
kable(df_lists_comb[1:2,],caption = "Combinations of hit vectors")
```
With `r dim(df_lists_comb)[1]` combinations. This includes self-references, which will be removed in the next step. But the important part of this next step is to map the is_event function over the hit vectors. Essentially, this is saying that we have an "event" - that is, a coincidence of the two plants at a single site - at any location where the two hit vectors contain a "1"; and the sum of that logical AND across the vectors counts how many such events occurred. For the first time here, I'm introducing the idea that the events we are interested in are the conjunctions of two plants; and these are binary events.

In the same way, though, that we are not interested in conjunctions that never occcur, we are also not interested in conjunctions that always occur, so we filter to remove both non-events and universal events. There is no need to retain the "events" vectors.

```{r}
is_event <- function(...){ # See map2_int below
  sum(..1 & ..2)
}

df_lists_comb_as <- df_lists_comb %>% 
  filter(var != var2) %>% 
  mutate(events = map2_int(.x = vector, .y = vector2, .f = is_event)) %>% 
  filter((events > 0) & events < length(df_lists$vector[[1]])) %>%
  select(-events)
rm(is_event)
```
From the `r dim(df_lists_comb)[1]` pairs of vectors in df_lists_comb we have now whittled down to `r dim(df_lists_comb_as)[1]` items representing "events" in df_lists_comb_as.

### Data wrangling: contingency tables
The table function encapsulated in xtab (see code below) makes it surprisingly easy to make contingency tables from the hit vectors;and map (or in this case map2) makes the process fast.
```{r}
# function to make the contingency tables.
xtab <- function(...){ # See map2 below
  t <- table(..1, ..2) 
}
# And map them into a convenient data structure.
df_tables <- df_lists_comb_as %>% 
  mutate(xtable = map2(vector, vector2, xtab))
# Clean up...
rm(df_lists, df_lists_comb, df_lists_comb_as, xtab)
kable(df_tables[1:8,c(1,3,5)],caption = "Sample of contingency tables")
```
The contingency tables have dimension (2,2) and thus 86,65,1,6 is to be read:

         0|1
      ----|----
      0|86|65
      ---------
      1| 1| 6

### Data wrangling: the first graph.
At this point in the analysis, the contingency tables can be regarded as attributes of the relationship between two interacting species. We can start thinking about a graph (in the graph-theoretical sense) with vertices or nodes as the species and the contingency tables belonging to the links or edges between them, and this thought now starts to be conceptually useful.

For one thing, we have twice as many links as we need. The first entry in the table is Achillea_millefolium/Achillea_ptarmica. Some hundreds of entries further down we would find Achillea_ptarmica/Achillea_millefolium. Since we are about to do some time consuming maths we want to avoid this duplication. Programmatically the most transparent approach is to represent the table as a graph, and simplify it to remove the duplicates; the routines for this kind of thing are built into the graph-theoretical libraries.
```{r}
# Make a graph and simplify it before doing the fisher test
df_edges <- df_tables %>% select(var, var2) %>% rename(from = var, to = var2)
g1 <- as_tbl_graph(graph.data.frame(d = df_edges, directed = FALSE))

# Add the contingency tables as edge attributs
g1 <- g1 %>% activate("edges") %>%
  mutate(xtable = df_tables$xtable)

# Simplify the Tidygraph way
g1 <- g1 %>% activate("edges") %>% 
  filter(!edge_is_multiple()) %>%
  filter(!edge_is_loop())
# Clean up ...
rm(df_edges, df_tables)
```

### Data wrangling: Fisher test

```{r}
# Important function to SAFELY apply fisher.test, ensuring we get
# some return value for diad.
sft <- safely(.f = fisher.test, otherwise = NULL, quiet = TRUE)

# Map fisher.test over dyads
g1 <- g1 %>% activate("edges") %>%
  mutate(f_test = map(xtable, sft))
rm(sft)
```
Believe me, after this code block the edges have the Fisher test result attached. The problem is to extract the information of importance - the p-values and the odds-ratios - from the data structure returned by the code. finding a map function to do this has defeated me so I resort to a for-loop.
```{r attach_stats_to graph, warning=FALSE}
# Temporary data structures needed to extract fisher.test statistics
# from <hresult> class and transfer them to edges data where they belong.
x <- tibble(pval = rep(0, gsize(g1)))
x <- x %>% mutate(lor = pval)
df_t <- g1 %>% activate("edges") %>% as_tibble()

# Extract p.val and Odds Ratio from fisher.test <hresult>
# Would prefer to do using map.
for(i in 1:gsize(g1)){ 
  x$pval[i] <- (ifelse(is.null(df_t[["f_test"]][[i]][["result"]][["p.value"]]), 100, df_t[["f_test"]][[i]][["result"]][["p.value"]]))
  x$lor[i] <- (ifelse(is.null(df_t[["f_test"]][[i]][["result"]][["p.value"]]), 100, df_t[["f_test"]][[i]][["result"]][["estimate"]]))
}
# Clean up...
rm(i, df_t)

# Actually transfer the stats to the edge attributes
g1 <- g1 %>%
  activate("edges") %>%
  mutate(pval = x$pval) %>% 
  mutate(lor = log(x$lor)) # NOTE log
# Clean up ...  
rm(x)

```
There are loads of duff results from the fisher test so it is not worth listing the result at this point. Better rather to proceed to the final and key step of all this data wrangling: sorting out just how much of all this we can reject. Essentially, rejecting any diad with p_val > 0.5, and values for the odds-ratio flagged as "Inf". It is useful here to examine the distribution of the selected (log)odds-ratios. Filtering the edges leaves many isolated nodes, so we remove those, and examine the hairball.

### Data wrangling: rejecting non-events
This is the key step where we apply our criteria (p-val < 0.05, finite odds-ratio) and thereby reduce the data set to a manageable size.
```{r hairball, warning=FALSE}
##### KEY STEP IN ANALYISIS. #####
# Select filters on pval and lor
# IMPORTANT NOTE SBM is told nothing about pval or lor. 
# They are used only to select dyads for the SBM to work on.
g1 <- g1 %>%
  activate("edges") %>%
  filter(pval < 0.05) %>% 
  filter(is.finite(lor)) %>%
  mutate(weight = abs(lor)) %>%
  mutate(sgn = ifelse(lor > 0, "associative", "dissociative"))
# Remove isolated nodes
g1 <- g1 %>% activate("nodes") %>% filter(degree(g1) > 0)

# Draw the graph
g1 %>% ggraph() +
  geom_edge_link(colour = "grey80") + 
  geom_node_point(colour = "tomato") +
  ggtitle("Current state of the graph") +   
  theme_graph()
```
The hairball shows that the graph is reasonably tractable at this point; it is also useful to check on the (log) odds-ratio histogram.
```{r lor_histogram, warning=FALSE}
# Check on the lor histogram
df <- g1 %>% activate("edges") %>% as_tibble()
plot(ggplot(df, aes(lor))  +
       geom_histogram(aes(y = ..density..), binwidth = 0.25, colour = "black") +
       geom_vline(xintercept = 0, colour = "red") +
       xlim(-6, 8) +
       ylim(0.0, 0.6) +
       labs(title ="", x = "log(odds ratio)"))
rm(df)

```

## Model building.
```{r}
# Obtain the adjacency matrix ...
M <- as_adj(g1, type = "both", sparse = F)

# And finally, build the model!
the_model <- estimateSimpleSBM(M, 'bernoulli', estimOptions = list(plot = T )) #TRUE))
# NOTE: ICL Integrated Completed Likelihood: figure of merit for the multiple
# the_model explored during estimation
## NOTE to locate the model: model <- which.max(the_model$ICL) # Select the model to use
rm(M) # Clean up...

# Print C matrix. Note that it can be recovered from the_model$connectParam
print(as_tibble(the_model$connectParam))

# Add group memberships to the node properties
g1 <- g1 %>% activate("nodes") %>% mutate(group = the_model$memberships)

# Generate block plot...
# Add EDGE group membership, NA for edges between blocks.
# Group assigned only to edges between diads within a block.
g1 <- g1 %>%
  activate(nodes) %>%
  morph(to_split, group) %>%
  activate(edges) %>%
  mutate(edge_group = as.character(.N()$group[1])) %>%
  unmorph()

# Matrix plot. Points are EDGES. Axes are NODES, i.e plants.
# Edges link the plant represented on the vertical axis to the
# corresponding plant on the horizontal axis.
plot(ggraph(
  g1, 'matrix', sort.by = group) +
    geom_edge_point(aes(colour = edge_group), mirror = TRUE, edge_size = 3) +
    scale_y_reverse() +
    coord_fixed() +
    labs(edge_colour = 'group') +
    ggtitle("SBM"))

```




## Stochastic Block Model


## Conclusions
1. Stochastic Block Models have been used to summarise the data from a survey of mesotrophic grassland in the catchment of the Sussex Ouse.
2. Despite the combinatorial complexity of the data, useful results were obtained without arbitrary preconditions.
3. The two most useful models incorporated, respectively, associative and dissociative interactions together, and dissociative interactions only.
4. SBM identifies negative groups (in-group probability less than out-group probability) as well as positive groups. This is conceptually a different phenomenon from the idea of associative/dissociative interactions, and independent from it. Dissociative diads are frequently placed in the same (positive) group.
5. SBM identifies joint group memberships (group membership is not exclusive).
6. The groups that SBM finds exist in the data, they are not necessarily expressed in the field.
7. With these insights we can make rational decisions about simplifying future analyses; in particular it makes sense to exclude the 86 species in group 7. We can also make rational decisions about where to focus our attention.
8. It will be of interest to examine the group-wise distributions of particular kinds of plant, for example grasses, legumes, hemiparasites.




